{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=12 color=red>Scaling Data with Limited Memory</font>\n",
    "\n",
    "# `Mr Fugu Data Science`\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os, gc # gc is used for garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Considerations:`\n",
    "\n",
    "+ Loading data into memory can be tricky and problematic if you have large datasets. Taking this into account is important and techniques used should be evaluated in order to be efficient with processing.\n",
    "\n",
    "**Ex. )** `Pandas` **will load all your data into memory** causing issues if you have a dataset larger than the memory you have available for your machine.\n",
    "\n",
    "* Pay attention to your data, you may be making (intermediate) copies of the data and slowing down processing!\n",
    "    + `Pandas isn't the best choice for all situations`\n",
    "\n",
    "+ We can:\n",
    "    + Buy a faster machine or rent access like cloud computing\n",
    "    + You can try to offload from memory onto disk, but this is slower than RAM\n",
    "\n",
    "`Hmm. What do we do and how to approach this issue?`\n",
    "\n",
    "\n",
    "`--------------------------------------`\n",
    "\n",
    "**Have you ever used a dataset and wondered where did my memory go?**\n",
    "+ I mean, you have a small file maybe 3GB and while doing manipulations and for some reason your data have grown by a few GB.\n",
    "\n",
    "https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A good recommendation would be to figure out how much memory your using before/during your analysis as a reference\n",
    "\n",
    "* **`Profiling:`**: can be a measure used to aid in investigating your memory usuage\n",
    "\n",
    "\n",
    "`------------------------------------`\n",
    "\n",
    "* **Alternate option to consider:**\n",
    "    * **`Mac/Linux`**: consider a library `from resource import getrusage, RUSAGE_SELF`\n",
    "    * **`Windows:`** try using `import psutil`\n",
    "    \n",
    "\n",
    "`------------------------------------`\n",
    "    \n",
    "* **`Terminal/Command Prompt:`** consider `htop` command\n",
    "    \n",
    "`------------------- Possible Reading & Insight ------------------------------`\n",
    "\n",
    "\n",
    "https://pythonspeed.com/articles/estimating-memory-usage/\n",
    "\n",
    "https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Ex.) \n",
    "\n",
    "https://www.geeksforgeeks.org/memory-profiling-in-python-using-memory_profiler/\n",
    "\n",
    "https://medium.com/zendesk-engineering/hunting-for-memory-leaks-in-python-applications-6824d0518774\n",
    "\n",
    "https://medium.com/codex/profiling-your-code-4a1538afd1e1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_01 = pd.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_.info(memory_usuage=?) # need to view parameters!\n",
    "\n",
    "# to convert into megabytes we need to multiply by 1*e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for alternative:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Profiling:\n",
    "\n",
    "# from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "# `Next, Consider Modeling/Estimating` your memory usage\n",
    "\n",
    "This will help you figure out where you may be eating up the memory and slowing everything down.\n",
    "\n",
    "+ But, what if you are stuck here and can't run till complete for a task/model?\n",
    "    + `Well, we have to consider using smaller models, possibly sample the data or even change the number of inputs. From there try to extrapolate your data and get a general idea of how much memory you may be using.`\n",
    "    \n",
    "    (**Sampling can bring you to a point where you may lose information, take this into account**)\n",
    "    \n",
    "`------------------------------------`\n",
    "    \n",
    "**Caution:** there is a difference between `Peak Resident Memory & Peak Memory Usuage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Compressing Data (Managing Resources Locally)`\n",
    "\n",
    "+ Changing Datatypes\n",
    "+ Chunking\n",
    "+ Indexing\n",
    "\n",
    "**`We do need to think about something, losing information for the sake of compression`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking/Loading: good if you only want to load data once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Data-Types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing: good if you want to access many times, `side note`: good with databases for fast access\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Using Different Libraries`\n",
    "\n",
    "+ Parallel Computing options such as `Dask` & `Vaex`\n",
    "    + Learn their limitations, because there are some!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting file types:\n",
    "\n",
    "+ consider converting your file to a lower memory usuage type such as:\n",
    "\n",
    "**`File Storage:`**\n",
    "\n",
    "* `Pickle Files`\n",
    "* `Parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Pickle Files:`\n",
    "\n",
    "+ easily read by Python and stores all of your data as well as datatypes, unlike a csv while saving memory.\n",
    "\n",
    "(**do NOT un-pickle a file you are not sure of its source due to coruption issues!**)\n",
    "\n",
    "https://towardsdatascience.com/the-power-of-pickletools-handling-large-model-pickle-files-7f9037b9086b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex.) compare Pickle file vs same CSV file size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet file:\n",
    "\n",
    "+ This can be a good option when you have an explosion of data for `pd.concat` for example is a memory hog.\n",
    "    + Storing on disk can be a better option in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Changing Data Types & Watch Out For Functions in Pandas:`\n",
    "\n",
    "\n",
    "* Changing Data Types to save memory:\n",
    "    * \n",
    "\n",
    "* `pandas.concat():` will explode the memory usuage so be aware of that. \n",
    "    * Solution: well if you can write to disk then do so with something such as a `parquet file` this is readable by pandas and you are able to store more on disk than RAM. **There is a tradeoff though** `time`. You will save memory usuage but sacrifce time. Depending on the task you should beaware of this!\n",
    "    \n",
    "    \n",
    "By default `Pandas` will infer your datatype and in doing so, will create large usuage of memory.\n",
    "    \n",
    "`Consider reading these:`\n",
    "    \n",
    "https://arrow.apache.org/docs/python/parquet.html\n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/\n",
    "\n",
    "https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Do you need all of the data?`\n",
    "\n",
    "`We want to minimize resources used and maximize our time!` ¯\\_(ツ)_/¯\n",
    "\n",
    "+ Can you shrink the dataset?\n",
    "    + Such as `dropping columns`\n",
    "    + Think about `garbage collection` such as `import gc` and use `gc.collect()`\n",
    "    + Consider `generator` functions\n",
    "\n",
    "`Garbage Collection:` can effectively aid in saving memory if you take control of it. Python in general will store everything after it is loaded into RAM or even when variables are created. \n",
    "    \n",
    "https://towardsdatascience.com/python-garbage-collection-article-4a530b0992e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Pandas Concat & Memory Issues:`\n",
    "\n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-----------------------------------------`\n",
    "\n",
    "# Lastly, Consider tools such as:\n",
    "+ Cloud Computing\n",
    "+ Google Colab\n",
    "+ Using a Cluster for parallel computing if your datasets are very large\n",
    "    + `Dask` & `Vaex` or even `Spark` may be an option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Citations & Help`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "[pandas thoughts](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#:~:text=pandas%20provides%20data%20structures%20for,need%20to%20make%20intermediate%20copies) | [Memory and Speed Python](https://pythonspeed.com/memory/) | [Further Help Kaggle](https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets)\n",
    "\n",
    "https://towardsdatascience.com/what-to-do-when-your-data-is-too-big-for-your-memory-65c84c600585\n",
    "\n",
    "https://www.ionos.com/digitalguide/hosting/technical-matters/in-memory-databases/\n",
    "\n",
    "https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask\n",
    "\n",
    "https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets\n",
    "\n",
    "https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html\n",
    "\n",
    "https://annefou.github.io/metos_python/07-LargeFiles/\n",
    "\n",
    "https://www.youtube.com/watch?v=HNE0qHJ9A9o\n",
    "\n",
    "https://pythonspeed.com/articles/estimating-memory-usage/\n",
    "\n",
    "https://datascience.stackexchange.com/questions/27767/opening-a-20gb-file-for-analysis-with-pandas\n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/\n",
    "\n",
    "https://docs.dask.org/en/latest/best-practices.html#store-data-efficiently\n",
    "\n",
    "https://docs.dask.org/en/latest/best-practices.html\n",
    "\n",
    "https://towardsdatascience.com/python-garbage-collection-article-4a530b0992e3\n",
    "\n",
    "`---------------------`\n",
    "\n",
    "https://realpython.com/python-mmap/ (memory mapping)\n",
    "\n",
    "https://towardsdatascience.com/optimized-i-o-operations-in-python-194f856210e0 (optimizing I/O)\n",
    "\n",
    "https://www.codementor.io/@satwikkansal/python-practices-for-efficient-code-performance-memory-and-usability-aze6oiq65\n",
    "\n",
    "https://towardsdatascience.com/reduce-memory-usage-and-make-your-python-code-faster-using-generators-bd79dbfeb4c\n",
    "\n",
    "https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
