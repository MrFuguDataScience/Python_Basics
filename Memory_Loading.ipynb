{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=12 color=red>Scaling Data with Limited Memory</font>\n",
    "\n",
    "# `Mr Fugu Data Science`\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Considerations:`\n",
    "\n",
    "+ Loading data into memory can be tricky and problematic if you have large datasets. Taking this into account is important and techniques used should be evaluated in order to be efficient with processing.\n",
    "\n",
    "**Ex. )** `Pandas` **will load all your data into memory** causing issues if you have a dataset larger than your memory.\n",
    "\n",
    "* Pay attention to your data, you may be making (intermediate) copies of the data and slowing down processing!\n",
    "    + `Pandas isn't the best choice for all situations`\n",
    "\n",
    "+ We can:\n",
    "    + Buy a faster machine or rent access like cloud computing\n",
    "    + You can try to offload from memory onto disk, but this is slower than RAM\n",
    "\n",
    "`Hmm. What do we do and how to approach this issue?`\n",
    "\n",
    "\n",
    "`--------------------------------------`\n",
    "\n",
    "**Have you ever used a dataset and wondered where did my memory go?**\n",
    "+ I mean, you have a small file maybe < 1GB and while doing manipulations your data have grown by a few GB's.\n",
    "\n",
    "https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A good recommendation would be to figure out how much memory your using before/during your analysis as a reference\n",
    "\n",
    "* **`Profiling:`**: can be a measure used to aid in investigating your memory usuage\n",
    "\n",
    "https://www.geeksforgeeks.org/memory-profiling-in-python-using-memory_profiler/\n",
    "\n",
    "https://medium.com/zendesk-engineering/hunting-for-memory-leaks-in-python-applications-6824d0518774\n",
    "\n",
    "https://medium.com/codex/profiling-your-code-4a1538afd1e1\n",
    "\n",
    "`------------------------------------`\n",
    "\n",
    "* **Alternate option to consider:**\n",
    "    * **`Mac/Linux`**: consider a library `from resource import getrusage, RUSAGE_SELF`\n",
    "    * **`Windows:`** try using `import psutil`\n",
    "    \n",
    "\n",
    "`------------------------------------`\n",
    "    \n",
    "* **`Terminal/Command Prompt:`** consider `htop` command\n",
    "    \n",
    "`------------------- Possible Reading & Insight ------------------------------`\n",
    "\n",
    "\n",
    "https://pythonspeed.com/articles/estimating-memory-usage/\n",
    "\n",
    "https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Profiling:`** \n",
    "\n",
    "+ Consider needing to find the time certain operations run in a script, NOT benching marking. This is because you are running statistics on the entire program.\n",
    "    + In that case you would want to use something such as \"timeit\"\n",
    "\n",
    "`Two types of Profiling:`\n",
    "\n",
    "+ **Deterministic:**\n",
    "    * monitoring events, while being accurate will have an effect on performance overhead. This would be better run on small functions or operations.\n",
    "\n",
    "+ **Statistical:**\n",
    "    * Less accurate but also uses fewer overhead resources by taking samples.\n",
    "    \n",
    "Something really useful and pretty cools is the (Call Graph) look into **gprof2dot** for example. It will convert your script into a graph like structure showing what functions are calling each other.\n",
    "\n",
    "\n",
    "# `Other Tools:`\n",
    "\n",
    "    + vprof\n",
    "    + pyflame\n",
    "    + stackImpact\n",
    "    \n",
    "https://medium.com/@antoniomdk1/hpc-with-python-part-1-profiling-1dda4d172cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "# `Next, Consider Modeling/Estimating` your memory usage\n",
    "\n",
    "This will help you figure out where you may be eating up the memory and slowing everything down.\n",
    "\n",
    "+ But, what if you are stuck here and can't run till complete for a task/model?\n",
    "    + `Well, we have to consider using smaller models, (possibly sample the data or even change the number of inputs).` \n",
    "    + From there try to extrapolate your data and get a general idea of how much memory you may be using.\n",
    "    \n",
    "    (**Sampling can bring you to a point where you may lose information, take this into account**)\n",
    "    \n",
    "`------------------------------------`\n",
    "    \n",
    "**Caution:** there is a difference between `Peak Resident Memory & Peak Memory Usuage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `(Managing Resources Locally)`\n",
    "\n",
    "+ `Think about garbage collection:` this can be handy if you are doing a lot of transformations or creation/deletion of objects. \n",
    "    + Mac: htop function\n",
    "    + PC: psutil\n",
    "\n",
    "+ **Changing Datatypes:** this can drastically change memory requirements with minimal work\n",
    "\n",
    "     + **Chunking:** ingesting your data as the name implies \"by chunks\"\n",
    "     \n",
    "        + **Indexing:** useful if you need subsets of the data\n",
    "        \n",
    "            + **Compression:** there are two forms so be careful \"lossless\" and \"lossy\"\n",
    "\n",
    "**`We do need to think about something, losing information for the sake of compression`** \n",
    "\n",
    "https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets (good read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Using Different Libraries`\n",
    "\n",
    "+ Parallel Computing options such as `Dask` & `Vaex`\n",
    "\n",
    "`Dask:` scaling for clusters\n",
    "\n",
    "`Vaex:` work with big data on a single machine\n",
    "\n",
    "    + Learn their limitations, because there are some!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting file types:\n",
    "\n",
    "+ consider converting your file to a lower memory usuage type such as:\n",
    "\n",
    "**`File Storage:`**\n",
    "\n",
    "* `Pickle Files`\n",
    "* `Parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Pickle Files:` used to store data in byte format (serialize/deserialize)\n",
    "\n",
    "`You can take that formatting and convert it back for python to read. But, you have to be care full of memory usuage exploding going in reverse. There are tools to help you manage memory such as (pickletools).` \n",
    "    + Pickling is used to transfer data in a manageable way, in the real world.\n",
    "    \n",
    "    --------------------------\n",
    "\n",
    "+ `easily read by Python and stores all of your data as well as datatypes, unlike a csv while saving memory.`\n",
    "    * CSV doesn't store information such as datatypes.\n",
    "\n",
    "(**do NOT un-pickle a file you are not sure of its source due to coruption issues!**)\n",
    "\n",
    "https://towardsdatascience.com/the-power-of-pickletools-handling-large-model-pickle-files-7f9037b9086b\n",
    "\n",
    "https://analyticsindiamag.com/complete-guide-to-different-persisting-methods-in-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Background Motivation:`**\n",
    "\n",
    "`File formats such as JSON, CSV and databases such as PSQL use row-wise storing. This isn't necessarily what you want when you have files in the GB ranges for read/write operations on a single machine.`\n",
    "\n",
    "Working with columns allows you flexibility calling only the data you want and saving memory. \n",
    "    + If you can flatten columns with redundant information you can efficiently manage memory (with a column) oriented scheme.\n",
    "    \n",
    "    \n",
    "`-----------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Parquet file:` *(helps with storage size)*\n",
    "\n",
    "\n",
    "+ This can be a good option when you want to:\n",
    "    * Supports nested data\n",
    "    * Save datatype information\n",
    "    * Faster to read than csv and supported by Hadoop\n",
    "    * Great for running queries on big data (GB for each file), reduces overhead costs vs csv files\n",
    "    * Supports various encodings, \n",
    "    \n",
    "`3 ways Parquet can help you:`\n",
    "\n",
    "1. ) **Columnar Storage**\n",
    "    * Build for efficiency vs CSV files format\n",
    "\n",
    "2. ) **Columnar Compression**\n",
    "    * able to use different codecs for different files aiding in compression and helps with query speed\n",
    "\n",
    "3. ) **Data Partitioning**\n",
    "    * This is done by spliting on unique values and crating a tree structure, like key-value pairs.\n",
    "\n",
    "**Reading data such as csv or json are done row-wise but, this has an inherent problem when reading in large files which are usually iteratively!** `Therefore, we have the columnar approach which reduces our overhead by storing data column-wise. allowing you to use only the columns you need`\n",
    "    \n",
    "    \n",
    "https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d (GOOD Reading)\n",
    "\n",
    "https://blog.datasyndrome.com/python-and-parquet-performance-e71da65269ce (inspiration for this cell)\n",
    "\n",
    "https://docs.microsoft.com/en-us/answers/questions/381106/parquet-in-data-lake-vs-csv-file.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Comparing:` look at (game column)\n",
    "\n",
    "\n",
    "| Some_ID \t| Pokemon_Card \t| Game_Number \t| Dollar_Value \t|\n",
    "|---------\t|--------------\t|-------------\t|--------------\t|\n",
    "| 3       \t| Charizard    \t| S4a         \t| 200          \t|\n",
    "| 2       \t| Pikachu      \t| S4a         \t| 5            \t|\n",
    "| 4       \t| Eevee        \t| S4a         \t| 30           \t|\n",
    "| 1       \t| Charmander   \t| S4a         \t| 10           \t|\n",
    "| 6       \t| Skylar       \t| S4a         \t| 15           \t|\n",
    " \n",
    " \n",
    " + We would have to go line by line (row-wise) and find redundant values\n",
    " \n",
    " + Whereas, if we are using column based approaches we can flatten these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`----------------------------------------------------------------------------------`\n",
    "\n",
    "# `Other Ways To Save Memory`\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "\n",
    "# 1.) **Changing Data Types to save memory**\n",
    "   \n",
    "\n",
    "`-------------------------------------------------`\n",
    "\n",
    "# 2.) `Watch out for pandas.concat(): will explode the memory usuage so be aware of that. `\n",
    "\n",
    "   **Solution**: if you can write to disk then do so with something such as a `parquet file`. This is readable by pandas and you are able to store more on disk, since you are RAM limited. **There is a tradeoff though** `time`. You will save memory usuage but, sacrifce time. Depending on the task you should beaware of this!\n",
    "    \n",
    "    \n",
    "By default `Pandas` will infer your datatype and in doing so, will create large usuage of memory.\n",
    "    \n",
    "`Consider reading these:`\n",
    "    \n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/\n",
    "\n",
    "https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Do you need all of the data?`\n",
    "\n",
    "`We want to minimize resources used and maximize our time!` ¯\\_(ツ)_/¯\n",
    "\n",
    "+ Can you shrink the dataset?\n",
    "    + Such as `dropping columns`\n",
    "    + Think about `garbage collection` such as `import gc` and use `gc.collect()`\n",
    "    + Consider `generator` functions\n",
    "\n",
    "`Garbage Collection:` can effectively aid in saving memory if you take control of it. Python in general will store everything after it is loaded into RAM or even when variables are created. \n",
    "    \n",
    "https://towardsdatascience.com/python-garbage-collection-article-4a530b0992e3\n",
    "\n",
    "https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Pandas Concat & Memory Issues:`\n",
    "\n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-----------------------------------------`\n",
    "\n",
    "# Lastly, Consider tools such as:\n",
    "+ Cloud Computing\n",
    "+ Google Colab\n",
    "+ Using a Cluster for parallel computing if your datasets are very large\n",
    "    + `Dask` & `Vaex` or even `Spark` may be an option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Like,</font><font color=red> Share & SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Citations & Help`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "[pandas thoughts](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#:~:text=pandas%20provides%20data%20structures%20for,need%20to%20make%20intermediate%20copies) | [Memory and Speed Python](https://pythonspeed.com/memory/) | [Further Help Kaggle](https://www.kaggle.com/frankherfert/tips-tricks-for-working-with-large-datasets)\n",
    "\n",
    "https://towardsdatascience.com/what-to-do-when-your-data-is-too-big-for-your-memory-65c84c600585\n",
    "\n",
    "https://www.ionos.com/digitalguide/hosting/technical-matters/in-memory-databases/\n",
    "\n",
    "https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask\n",
    "\n",
    "https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets\n",
    "\n",
    "https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html\n",
    "\n",
    "https://annefou.github.io/metos_python/07-LargeFiles/\n",
    "\n",
    "https://www.youtube.com/watch?v=HNE0qHJ9A9o\n",
    "\n",
    "https://pythonspeed.com/articles/estimating-memory-usage/\n",
    "\n",
    "https://datascience.stackexchange.com/questions/27767/opening-a-20gb-file-for-analysis-with-pandas\n",
    "\n",
    "https://www.confessionsofadataguy.com/solving-the-memory-hungry-pandas-concat-problem/\n",
    "\n",
    "https://docs.dask.org/en/latest/best-practices.html#store-data-efficiently\n",
    "\n",
    "https://docs.dask.org/en/latest/best-practices.html\n",
    "\n",
    "https://towardsdatascience.com/python-garbage-collection-article-4a530b0992e3\n",
    "\n",
    "`---------------------`\n",
    "\n",
    "https://realpython.com/python-mmap/ (memory mapping)\n",
    "\n",
    "https://towardsdatascience.com/optimized-i-o-operations-in-python-194f856210e0 (optimizing I/O)\n",
    "\n",
    "https://www.codementor.io/@satwikkansal/python-practices-for-efficient-code-performance-memory-and-usability-aze6oiq65\n",
    "\n",
    "https://towardsdatascience.com/reduce-memory-usage-and-make-your-python-code-faster-using-generators-bd79dbfeb4c\n",
    "\n",
    "https://pythonspeed.com/articles/function-calls-prevent-garbage-collection/\n",
    "\n",
    "https://www.toucantoco.com/en/tech-blog/python-performance-optimization\n",
    "\n",
    "https://arrow.apache.org/docs/python/parquet.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
